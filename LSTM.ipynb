{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all songs\n",
    "songs = []\n",
    "for f in os.listdir(\"data/preprocessed\"):\n",
    "    songs.append(np.genfromtxt((\"data/preprocessed/%s\" % f), dtype=int, delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  105088\n"
     ]
    }
   ],
   "source": [
    "# Split data up into \"patterns\"\n",
    "pattern_length = 200\n",
    "data_X = []\n",
    "data_y = []\n",
    "for f in songs:\n",
    "    for i in range(0, len(f) - pattern_length, 1):\n",
    "        data_X.append(f[i:i+pattern_length])\n",
    "        data_y.append(f[i+pattern_length])\n",
    "n_patterns = len(data_X)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unused notes\n",
    "freq = np.bincount(np.array(data_X).flatten())\n",
    "\n",
    "# Get indices of nonzero frequencies\n",
    "non_zero_freq = np.nonzero(freq)[0]\n",
    "\n",
    "# Remember highest and lowest used notes\n",
    "lowest = non_zero_freq[0]\n",
    "highest = non_zero_freq[len(non_zero_freq) - 1]\n",
    "n_notes = highest - lowest + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X\n",
    "X = np.reshape(data_X, (n_patterns, pattern_length, 1))\n",
    "\n",
    "# Normalize\n",
    "X = (X - lowest) / n_notes\n",
    "\n",
    "# One hot encode\n",
    "y = np_utils.to_categorical(data_y - lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=((pattern_length, 1)), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_notes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "checkpoint = ModelCheckpoint(\"checkpoint-{epoch:02d}.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "102414/102414 [==============================] - 9918s 97ms/step - loss: 2.8566 - categorical_accuracy: 0.4037\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.85665, saving model to checkpoint-01.hdf5\n",
      "Epoch 2/40\n",
      "102414/102414 [==============================] - 9861s 96ms/step - loss: 2.6801 - categorical_accuracy: 0.4044\n",
      "\n",
      "Epoch 00002: loss improved from 2.85665 to 2.68013, saving model to checkpoint-02.hdf5\n",
      "Epoch 3/40\n",
      "102414/102414 [==============================] - 9792s 96ms/step - loss: 2.3307 - categorical_accuracy: 0.4141\n",
      "\n",
      "Epoch 00003: loss improved from 2.68013 to 2.33073, saving model to checkpoint-03.hdf5\n",
      "Epoch 4/40\n",
      "102414/102414 [==============================] - 9794s 96ms/step - loss: 2.2231 - categorical_accuracy: 0.4222\n",
      "\n",
      "Epoch 00004: loss improved from 2.33073 to 2.22309, saving model to checkpoint-04.hdf5\n",
      "Epoch 5/40\n",
      "102414/102414 [==============================] - 9871s 96ms/step - loss: 2.1561 - categorical_accuracy: 0.4281\n",
      "\n",
      "Epoch 00005: loss improved from 2.22309 to 2.15609, saving model to checkpoint-05.hdf5\n",
      "Epoch 6/40\n",
      "102414/102414 [==============================] - 9918s 97ms/step - loss: 2.0972 - categorical_accuracy: 0.4377\n",
      "\n",
      "Epoch 00006: loss improved from 2.15609 to 2.09725, saving model to checkpoint-06.hdf5\n",
      "Epoch 7/40\n",
      "102414/102414 [==============================] - 9891s 97ms/step - loss: 2.0415 - categorical_accuracy: 0.4461\n",
      "\n",
      "Epoch 00007: loss improved from 2.09725 to 2.04152, saving model to checkpoint-07.hdf5\n",
      "Epoch 8/40\n",
      "102414/102414 [==============================] - 9851s 96ms/step - loss: 1.9918 - categorical_accuracy: 0.4521\n",
      "\n",
      "Epoch 00008: loss improved from 2.04152 to 1.99179, saving model to checkpoint-08.hdf5\n",
      "Epoch 9/40\n",
      "102414/102414 [==============================] - 9832s 96ms/step - loss: 1.9458 - categorical_accuracy: 0.4599\n",
      "\n",
      "Epoch 00009: loss improved from 1.99179 to 1.94583, saving model to checkpoint-09.hdf5\n",
      "Epoch 10/40\n",
      "102414/102414 [==============================] - 9835s 96ms/step - loss: 1.8933 - categorical_accuracy: 0.4680\n",
      "\n",
      "Epoch 00010: loss improved from 1.94583 to 1.89327, saving model to checkpoint-10.hdf5\n",
      "Epoch 11/40\n",
      "102414/102414 [==============================] - 9867s 96ms/step - loss: 1.8492 - categorical_accuracy: 0.4746\n",
      "\n",
      "Epoch 00011: loss improved from 1.89327 to 1.84919, saving model to checkpoint-11.hdf5\n",
      "Epoch 12/40\n",
      "102414/102414 [==============================] - 9920s 97ms/step - loss: 1.8106 - categorical_accuracy: 0.4825\n",
      "\n",
      "Epoch 00012: loss improved from 1.84919 to 1.81061, saving model to checkpoint-12.hdf5\n",
      "Epoch 13/40\n",
      "102414/102414 [==============================] - 9933s 97ms/step - loss: 1.7674 - categorical_accuracy: 0.4893\n",
      "\n",
      "Epoch 00013: loss improved from 1.81061 to 1.76741, saving model to checkpoint-13.hdf5\n",
      "Epoch 14/40\n",
      "102414/102414 [==============================] - 9979s 97ms/step - loss: 1.7275 - categorical_accuracy: 0.4972\n",
      "\n",
      "Epoch 00014: loss improved from 1.76741 to 1.72748, saving model to checkpoint-14.hdf5\n",
      "Epoch 15/40\n",
      "102414/102414 [==============================] - 9925s 97ms/step - loss: 1.6898 - categorical_accuracy: 0.5048\n",
      "\n",
      "Epoch 00015: loss improved from 1.72748 to 1.68978, saving model to checkpoint-15.hdf5\n",
      "Epoch 16/40\n",
      "102414/102414 [==============================] - 9888s 97ms/step - loss: 1.6791 - categorical_accuracy: 0.5065\n",
      "\n",
      "Epoch 00016: loss improved from 1.68978 to 1.67912, saving model to checkpoint-16.hdf5\n",
      "Epoch 17/40\n",
      "102414/102414 [==============================] - 9904s 97ms/step - loss: 1.6179 - categorical_accuracy: 0.5188\n",
      "\n",
      "Epoch 00017: loss improved from 1.67912 to 1.61785, saving model to checkpoint-17.hdf5\n",
      "Epoch 18/40\n",
      " 80000/102414 [======================>.......] - ETA: 36:11 - loss: 1.5923 - categorical_accuracy: 0.5246"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-f500a2667303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "filename = \"checkpoint-17.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(data_X) - 1)\n",
    "pattern=X[start]\n",
    "song = []\n",
    "#Generate\n",
    "for i in range(200):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    prediction = model.predict(x, verbose=0).flatten()\n",
    "    index=np.argmax(prediction)\n",
    "    pattern = np.append(pattern, (index/float(n_notes)))\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    song = np.append(song, (index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 77.  77. 128.  72.  65.  77.  65.  65.  77. 128.  65.  65.  70.  65.\n",
      "  65.  72. 128.  65.  60.  72.  60.  60.  72. 128.  65.  60.  70.  60.\n",
      "  60.  72. 128.  65.  60.  70.  60.  60.  72. 128.  65.  60.  70.  60.\n",
      "  65.  72. 128.  65.  65.  72.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.  65.  77.  65.  72.  77. 128.  72.  65.  77.  65.\n",
      "  72.  77. 128.  72.]\n"
     ]
    }
   ],
   "source": [
    "print((pattern*n_notes) + lowest)\n",
    "np.savetxt(\"data/song.csv\", (song + lowest), fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
